# ğŸš€ Distributed Web Crawler

## ğŸ“Œ Overview

A **scalable distributed web crawler** built with **Python and Flask** that mimics how modern search engines like Google crawl, extract, and structure information from the web. This system uses multiple worker nodes coordinated through a central API to efficiently gather data and store it in structured format.

---

## ğŸ” What is a Web Crawler?

A **web crawler** (also known as a spider or bot) is an automated tool that systematically browses the internet to discover pages, extract information, and gather links for further crawling. It is the backbone of **search engines, AI data pipelines, market research tools, and cybersecurity systems**.

---

## ğŸŒ Project Purpose

This crawler simulates a **distributed architecture** similar to how large-scale web crawlers work in real-world applications. It automatically extracts:

* âœ… URL
* âœ… Page Title
* âœ… Page Size
* âœ… First Paragraph / Meaningful Content
* âœ… All Outbound Links

This information is saved into structured CSV files for **data analysis, research, AI training, or automation workflows**.

---

## ğŸ—ï¸ System Architecture

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Flask Coordinator API â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Node 1  â”‚             â”‚ Worker Node 2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 CSV Output Storage
```

---

## ğŸ› ï¸ Tech Stack

| Component     | Technology Used   |
| ------------- | ----------------- |
| Core Language | Python            |
| API Server    | Flask             |
| HTML Parsing  | BeautifulSoup     |
| HTTP Requests | Requests Library  |
| Concurrency   | Threading, Queues |
| Data Storage  | CSV Export        |

---

## ğŸ”§ Key Features

* ğŸŒ **Distributed Architecture** (multiple workers)
* âš¡ **Real-time Task Assignment via API**
* ğŸ“„ **Automatic Data Extraction** (Title, URL, Content, Links)
* ğŸ“ **CSV Export for Analytics**
* ğŸ”„ **Scalable and Extensible Design**

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/distributed-web-crawler.git
cd distributed-web-crawler
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start the Coordinator

```bash
python coordinator.py
```

### 4ï¸âƒ£ Start Worker Nodes

```bash
python worker.py
```

---

## ğŸ“Š Output Example (CSV)
<img width="975" height="655" alt="DWC 1" src="https://github.com/user-attachments/assets/64b24fe7-eb0b-42c7-9257-eeabfe4bfd45" />

---

## ğŸŒ Why This Matters

In the age of **AI, automation, and real-time data**, web crawlers are the foundation for:

* ğŸ¤– AI & LLM Training Datasets
* ğŸ“ˆ Business & Market Intelligence
* ğŸ” Cybersecurity & Compliance
* ğŸ§  Knowledge Graphs and Research

> **Data is the new fuel â€” and web crawlers are the engines that extract it.**

---

## âœ¨ Future Enhancements

* Integration with **Databases (MongoDB, Elasticsearch)**
* Support for **JavaScript-rendered Websites**
* Dashboard for **Real-Time Monitoring**
* Deployment as a **SaaS Platform** for Lead Generation & Analytics

---

## ğŸ¤ Contribute

Contributions are welcome! Feel free to fork this repository and submit a pull request.

---

## ğŸ“¬ Contact

If youâ€™re passionate about **AI, automation, or data engineering**, connect with me on LinkedIn or GitHub.

---

### â­ If you find this project valuable, donâ€™t forget to star the repository!
